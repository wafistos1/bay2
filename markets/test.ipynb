{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests pandas openpyxl selenium=3.14 fake_useragent bs4\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "@dataclass\n",
    "class Product:\n",
    "    link_url : str = ''\n",
    "    name : str = ''\n",
    "    price : str = ''\n",
    "    description : str = ''\n",
    "    base_image : str =  ''\n",
    "    add_image :list = field(default_factory=list)\n",
    "    cat1 : str = ''\n",
    "    cat2 : str = ''\n",
    "    cat3 : str = ''\n",
    "\n",
    "urls = pd.read_excel('path excel url file')\n",
    "list_urls = []\n",
    "for index, row in urls.iterrows():\n",
    "    list_urls.append(\n",
    "        {\n",
    "            'url': row['url'],\n",
    "            'cat1': row['cat1'],\n",
    "            'cat2': row['cat2'],\n",
    "            'cat3': row['cat3'],\n",
    "        }\n",
    "    )\n",
    "\n",
    "def scrape_data(url1):\n",
    "    cat1 = url1['cat1']\n",
    "    cat2 = url1['cat2']\n",
    "    cat3 = url1['cat3']\n",
    "    url = url1['url']\n",
    "    logging.info('URL: %s', url)\n",
    "    \n",
    "    return {\n",
    "        \n",
    "    }\n",
    "\n",
    "df = pd.read_excel('path excel url file')\n",
    "for i, url in enumerate(urls):\n",
    "    logging.info('Count: %s', i)\n",
    "    data = scrape_data(url)\n",
    "    df1 = pd.DataFrame([data])\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    df.to_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://vi-markets.com/%D8%A7%D8%B3%D8%AA%D8%A7%D9%86%D8%AF-%D8%A8%D8%B4%D9%83%D9%84-%D9%82%D9%81%D8%B5-%D8%AD%D8%AF%D9%8A%D8%AF-%D9%88%D8%AE%D8%B4%D8%A8-%D8%B1%D9%8A%D9%81%D9%8A-%D9%83%D8%A8%D9%8A%D8%B1/p1569067522'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "cookies = {'session': '134-8225175-0355220'}\n",
    "r = requests.get(url, headers=headers, cookies=cookies)\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('استاند بشكل قفص حديد وخشب ريفي كبير', '913.04')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = soup.find('h1', {'class': 'product-details__title'}).text.strip()\n",
    "price = soup.find('span', {'class': 'product-price'}).text.replace('ر.س', '').strip()\n",
    "images = soup.find('div', {'id': 'sp-slider-cont'}).find_all('img')\n",
    "\n",
    "list_images = [ img['src'] for img in images]\n",
    "base_image = list_images[0]\n",
    "add_images = ','.join(list_images[1: ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://cdn.salla.sa/GjZEW/FHDSeb05cOx09SR62CTrSeYKoJ5Nb3v02O7Sa5U0.jpg',\n",
       " 'https://cdn.salla.sa/GjZEW/eyxwkiJM93P9ve3lzWFtnZBfzfECNn6eO17iod1D.jpg',\n",
       " 'https://cdn.salla.sa/GjZEW/GkcARdY1IGzv93YYtWjfxW4nIQ10ewnLVXxcHymC.jpg',\n",
       " 'https://cdn.salla.sa/GjZEW/T2uySDACGhnEADqjwSXmH2PsibkPILISjZCalCEV.jpg',\n",
       " 'https://cdn.salla.sa/GjZEW/V4wiJmGQLUJ2W9CaqXGfEOOBzjgILzcjNnUX1oEq.jpg']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = soup.find('div', {'id': 'sp-slider-cont'}).find_all('img')\n",
    "\n",
    "list_images = [ img['src'] for img in images]\n",
    "base_image = list_images[0]\n",
    "add_images = ','.join(list_images[1: ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
